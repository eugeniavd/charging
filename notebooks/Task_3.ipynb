{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a862e399",
   "metadata": {},
   "source": [
    "# Task 3 - Cluster Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790570fc",
   "metadata": {},
   "source": [
    "*To better understand what typical charging sessions look like, carry out a cluster\n",
    "analysis to provide management with a succinct report of archetypical charging events. Think of an\n",
    "appropriate trade-off between explainability and information content and try to come up with names\n",
    "for these clusters. What is the value of identifying different types of charging sessions?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4541dfb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.base import ClusterMixin\n",
    "import plotly.express as px\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import *\n",
    "import ast\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "RAND = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8569bbc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# open dataset\n",
    "data_clusters = pd.DataFrame(pd.read_csv(\"../datasets/df_charging_sessions_with_weather.csv\"))\n",
    "data_clusters['connectionTime'] = pd.to_datetime(data_clusters['connectionTime'], utc=True).dt.tz_convert('America/Los_Angeles')\n",
    "data_clusters['disconnectTime'] = pd.to_datetime(data_clusters['disconnectTime'], utc=True).dt.tz_convert('America/Los_Angeles')\n",
    "data_clusters['doneChargingTime'] = pd.to_datetime(data_clusters['doneChargingTime'], utc=True).dt.tz_convert('America/Los_Angeles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a9a592",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_clusters.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba14db67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('\\033[1m' + '\\033[01;04;38;05;23m' + 'DATA ON THE DATAFRAME' + '\\033[01;00;38;05;23m')\n",
    "print()\n",
    "print('\\033[1m' + '\\033[01;38;05;30m' + '1. Let\\'s examine the dataframe information:' + '\\033[0m')\n",
    "print()\n",
    "display(data_clusters.info())  # Display information about all columns\n",
    "print()\n",
    "print('\\033[1m' + '\\033[01;38;05;30m' + '2. Check for complete duplicates in the data:')\n",
    "duplicate_rows = data_clusters.drop(columns=['userInputs']).duplicated().sum()\n",
    "display(duplicate_rows) # Checking for complete duplicates\n",
    "print()\n",
    "print('\\033[1m' + '\\033[01;38;05;30m' + '3. Let\\'s output descriptive statistics:')\n",
    "display(data_clusters.describe().round(2))  # Display descriptive statistics for all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80525347",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Look at the user input \n",
    "row_data = data_clusters['userInputs'].iloc[63882]\n",
    "row_data_1 = data_clusters['userInputs'].iloc[1]\n",
    "print(row_data)\n",
    "print(row_data_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed50762",
   "metadata": {},
   "source": [
    "In the 'userInputs' column, there are empty lists and lists with data. Let's see how many empty lists we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e38ffe5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function that first checks and converts a string to a list\n",
    "def is_empty(lst):\n",
    "    if isinstance(lst, str):  # Check if the element is a string\n",
    "        try:\n",
    "            lst = ast.literal_eval(lst)  # Convert the string to a list\n",
    "        except (ValueError, SyntaxError):\n",
    "            return False  \n",
    "    return not lst\n",
    "    \n",
    "# Applying the function to each element in the 'userInputs' column\n",
    "empty_counts = data_clusters['userInputs'].apply(is_empty).sum()\n",
    "total_rows = len(data_clusters)\n",
    "\n",
    "# Calculating the percentage of empty lists\n",
    "empty_percentage = (empty_counts / total_rows) * 100\n",
    "\n",
    "print(f\"Number of empty lists: {empty_counts} from {total_rows}\")\n",
    "print(f\"Percentage of empty lists: {empty_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f79aa97",
   "metadata": {},
   "source": [
    "#### Data Preparation for Cluster Analysis\n",
    "\n",
    "For clustering we use only numerical features, but our data contains some categorical features. So the first step will be the data preparation. \n",
    "\n",
    "##### Deletion\n",
    "- id - delete, because number of the record doesn't influence on the quality of the charging session   \n",
    "- userID, stationID - delete, because we are not going to analyse the particolar users or stations, we are looking for general patterns \n",
    "- sessionID - redundant for clustering since each session is unique\n",
    "- spaceID - we have kept siteID to see charging in private and public places, but we haven't information to analyse particular places because their names are encoded                      \n",
    "- userInputs - we have 27% missing data, so to use presented data for clustering we will have to fill Nan with new values, but this could potentially distort the results of clustering, especially in algorithms sensitive to the distances between data points, such as K-means, hierarchical clustering\n",
    "\n",
    "##### Transformation\n",
    "- connectionTime - extract time, date, month and year\n",
    "- disconnectTime - extract time and date (we assumed that month and year are the same with connection, so we already have this information)\n",
    "- receive final plugged time (disconnectTime - connectionTime)\n",
    "- doneChargingTime - extract charging time for session (doneChargingTime - connectionTime) to receive real charging time\n",
    "- useless_parking - calculating parking time without charging (plugged time - charging time for session)\n",
    "\n",
    "##### Encoding into a numerical feature    \n",
    "- weather                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8fab47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Deletion\n",
    "data_clusters = data_clusters.drop(columns=['id', 'userID', 'sessionID', 'userInputs',\n",
    "                                            'spaceID', 'stationID'])\n",
    "#check\n",
    "data_clusters.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1865f941",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculating the plugging duration in minutes\n",
    "data_clusters['charging_duration_disconnect'] = (\n",
    "    (data_clusters['disconnectTime'] - data_clusters['connectionTime']).dt.total_seconds() / 60\n",
    ").astype(int)\n",
    "\n",
    "# Calculating the charging duration in minutes\n",
    "data_clusters['charging_duration_done'] = (\n",
    "    (data_clusters['doneChargingTime'] - data_clusters['connectionTime']).dt.total_seconds() / 60\n",
    ").astype(int)\n",
    "\n",
    "# Calculating parking time without charging\n",
    "data_clusters['useless_parking'] = (\n",
    "    (data_clusters['charging_duration_disconnect'] - data_clusters['charging_duration_done']))\n",
    "\n",
    "data_clusters.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525d8825",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extracting components from 'connectionTime' \n",
    "data_clusters['connection_year'] = data_clusters['connectionTime'].dt.year  # Extracting and adding year as a number\n",
    "data_clusters['connection_month'] = data_clusters['connectionTime'].dt.month  # Extracting and adding month as a number\n",
    "data_clusters['connection_day'] = data_clusters['connectionTime'].dt.day  # Extracting and adding day as a number\n",
    "data_clusters['connection_hour'] = data_clusters['connectionTime'].dt.hour  # Extracting and adding hour as a number\n",
    "\n",
    "# Extracting components from 'disconnectTime' \n",
    "data_clusters['disconnect_time'] = data_clusters['disconnectTime'].dt.hour  \n",
    "data_clusters['disconnect_date'] = data_clusters['disconnectTime'].dt.day  \n",
    "\n",
    "# Deletion 'connectionTime' and disconnectTime' \n",
    "data_clusters = data_clusters.drop(columns=['connectionTime', 'disconnectTime', 'doneChargingTime'])\n",
    "\n",
    "#check\n",
    "data_clusters.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ef733c",
   "metadata": {},
   "source": [
    "Now we will observe the data to see if we have outliers which could influence on the clustering results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a867ac4e",
   "metadata": {},
   "source": [
    "#### Outliers\n",
    "\n",
    "For each feature, we will construct pairs of histograms and box plots to more closely examine the distributions and identify outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22b4494",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to count a share of outliers\n",
    "def get_outliers(col: pd.Series, iqr_coeff=1.5):\n",
    "    # Calculate the first and third quartiles of the column\n",
    "    q25, q75 = col.quantile([.25, .75])\n",
    "    \n",
    "    # Calculate the interquartile range \n",
    "    distr_iqr = q75 - q25\n",
    "    \n",
    "    # Define the whiskers of the data, which are considered outliers\n",
    "    whiskers = pd.Series([q25 - iqr_coeff * distr_iqr, q75 + iqr_coeff * distr_iqr])\n",
    "    \n",
    "    # Clip the whiskers to handle cases where whiskers extend beyond the range of the data\n",
    "    whisker_low, whisker_upp = whiskers.clip(lower=col.min(), upper=col.max())\n",
    "    \n",
    "    # Calculate the ratio of outliers\n",
    "    outliers_ratio = ((col > whisker_upp).sum() + (col < whisker_low).sum()) / len(col)\n",
    "\n",
    "    return outliers_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81321b43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to plot the data + share of outliers\n",
    "def get_eda(df: pd.DataFrame, outliers=True, iqr_coeff=1.5):\n",
    "    '''\n",
    "    Plots histograms and boxplot diagrams for all numeric features \n",
    "\n",
    "    Parameters:\n",
    "    - df: The DataFrame containing the data.\n",
    "    - outliers: A boolean flag to indicate whether to calculate and display outlier percentage.\n",
    "    - iqr_coeff: The coefficient to calculate the IQR range for outlier detection.\n",
    "    '''\n",
    "    # Identify numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    num_features = len(numeric_cols)\n",
    "    \n",
    "    # Adjusting the figsize for better readability\n",
    "    fig, axs = plt.subplots(num_features, 2, figsize=(12, num_features * 4))  # Increased figsize for better fit\n",
    "    \n",
    "    # Define a medium font size for all text elements\n",
    "    medium_font_size = 10\n",
    "    plt.rcParams.update({'font.size': medium_font_size})\n",
    "\n",
    "    # Create plots for each numeric column\n",
    "    for idx, col in enumerate(numeric_cols):\n",
    "        # Histogram\n",
    "        axs[idx, 0].hist(df[col], bins=20, color='#35c0cd', edgecolor='black')\n",
    "        axs[idx, 0].set_ylabel('Count', fontsize=medium_font_size)\n",
    "        axs[idx, 0].set_xlabel(f'{col}', fontsize=medium_font_size)\n",
    "        axs[idx, 0].tick_params(labelsize=medium_font_size)\n",
    "\n",
    "        # Box plot\n",
    "        box = axs[idx, 1].boxplot(df[col], vert=True, widths=0.5, patch_artist=True, boxprops=dict(facecolor=\"#35c0cd\"))\n",
    "        axs[idx, 1].tick_params(labelsize=medium_font_size)\n",
    "\n",
    "        # Calculate and display the share of outliers if required\n",
    "        if outliers:\n",
    "            outlier_perc = get_outliers(df[col], iqr_coeff=iqr_coeff)\n",
    "            axs[idx, 1].set_title(f'{col} Outliers: {outlier_perc:.1%}', fontsize=medium_font_size)\n",
    "        else:\n",
    "            axs[idx, 1].set_title(f'{col} Boxplot', fontsize=medium_font_size)\n",
    "\n",
    "    # Adjust the layout of the plots to prevent overlapping\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ee7490",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_eda(df=data_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8c384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check also a standard deviation\n",
    "data_clusters.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504dbfc2",
   "metadata": {},
   "source": [
    "After studying the graphs, let's describe the features in more detail.\n",
    "\n",
    "- kWhDelivered: The majority of values are concentrated around zero with a rapid decline in frequency as the value increases. High values are rare and can be considered anomalous or outliers.\n",
    "\n",
    "- siteID: This feature has two categories, with no outliers.\n",
    "\n",
    "- temperature: The distribution of temperatures seems normal or close to normal with a slight skew to the left, indicating the presence of colder temperature values. Also there are some extreme temperature conditions.\n",
    "\n",
    "- pressure: The distribution has a shape close to normal, the presence of many separate points below the lower \"whisker\" indicates the presence of outliers in the data.\n",
    "\n",
    "- windspeed: There is a clear declining trend: the higher the wind speed, the fewer observations. The distribution is skewed to the right, with a long tail extending to higher speeds. The diagram also shows a significant number of outliers above the upper \"whisker,\" indicating the presence of observations with unusually high wind speeds.\n",
    "\n",
    "- precipitation: The bulk of the precipitation data is concentrated around the value of 0, suggesting that in most cases precipitation is absent or minimal. There are many points above the upper \"whisker\" on the box plot, representing outliers. This may indicate rare events of heavy precipitation.\n",
    "\n",
    "- charging_duration_disconnect: 75% of observations have a low duration of charging before disconnection. The \"whiskers\" of the box plot extend significantly beyond the upper quartile, showing the spread of the remaining 25% of the data. The box plot also shows individual points that are located significantly above the upper \"whisker.\" These points represent outliers.\n",
    "\n",
    "- charging_duration_done: The majority of charging sessions have a very short duration. There is a sharp drop in frequency with increasing charging duration, and the data show a right-skewed distribution with a long tail, indicating the presence of a small number of cases with lengthy charging.\n",
    "\n",
    "- useless_parking - also concentrated aroud zero, many outliers\n",
    "\n",
    "- connection_year - distribution between 4 values, 4 peaks visible.\n",
    "\n",
    "- connection_month, connection_day, disconnect_date: The distribution of observations is quite normal even with some variations in frequency between different months and dates of the month.\n",
    "\n",
    "- connection_hour: The distribution shows a peak at the beginning of the day (or at the end of the previous one), which may indicate increased connection activity at this time. \n",
    "\n",
    "- disconnect_time: The most common time of disconnection is between approximately 15 and 18 hours. There are several points representing outliers that are located below the lower \"whisker,\" possibly indicating disconnections occurring at unusually early hours.\n",
    "\n",
    "The features with much outliers an big standard deviation are: kWhDelivered, charging_duration_done, charging_duration_disconnect, useless_parking. Let's delete outliers because they could influence on clustering algorythms. \n",
    "\n",
    "Also we suggested to delete the features 'precipitation' (not so much information, precipitation is absent or minimal) and 'windspeed' (distribution is skewed, significant number of outliers), 'connection_day' and 'disconnection_day' (we concentrate on seasonal patterns, for this month and temperature are enough)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9122adb2-3e84-4a71-aacd-f5c61ce71934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete some features \n",
    "data_clusters = data_clusters.drop(['windspeed', 'precipitation', \n",
    "                                    'connection_day', 'disconnect_date'], \n",
    "                                   axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3927b224-d5bb-447b-8003-b112f4a9cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "data_clusters.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0d833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove outliers\n",
    "def remove_outliers(data, column):\n",
    "    Q3, Q1 = np.nanpercentile(data[column], [75, 25])\n",
    "    IQR = Q3 - Q1\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    filtered_data = data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]\n",
    "    \n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07947a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clusters_outliers = remove_outliers(data=data_clusters, column='kWhDelivered')\n",
    "data_clusters_outliers = remove_outliers(data=data_clusters, column='charging_duration_disconnect')\n",
    "data_clusters_outliers = remove_outliers(data=data_clusters, column='charging_duration_done')\n",
    "data_clusters_outliers = remove_outliers(data=data_clusters, column='useless_parking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb60b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "data_clusters_outliers.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81454d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_eda(df=data_clusters_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e33d64-8fa5-4cd8-9d56-3321dc30e06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_count = len(data_clusters)\n",
    "clean_row_count = len(data_clusters_outliers)\n",
    "deleted_rows = row_count - clean_row_count\n",
    "deleted_fraction = deleted_rows / row_count * 100\n",
    "print(f\"Number of rows after cleaning: {clean_row_count}\")\n",
    "print(f\"Fraction of rows deleted: {deleted_fraction:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a19fc1",
   "metadata": {},
   "source": [
    "In the graph, we can see that we have smoothly removed the outliers. We deleted around 1%, targeting only very rare cases, primarily because the algorithms we are going to use are sensitive to outliers. Let's also check the data for correlation, as this could also influence the clustering results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15be8921",
   "metadata": {},
   "source": [
    "##### Ð¡orrelation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aec882d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Correlation checking\n",
    "\n",
    "# Create a color map \n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Create a figure with two subplots \n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))  \n",
    "\n",
    "# Create a mask for the upper triangle for the first correlation matrix\n",
    "mask_corr1 = np.triu(np.ones_like(data_clusters.corr(numeric_only=True), dtype=bool))\n",
    "\n",
    "# Draw the heatmap for the first correlation matrix\n",
    "sns.heatmap(data_clusters.corr(numeric_only=True), mask=mask_corr1, cmap=cmap,\n",
    "            vmax=1, vmin=-1, square=True, linewidths=.7, cbar_kws={\"shrink\": .8}, ax=ax1)\n",
    "ax1.set_title('Correlation Matrix (with outliers).\\nPearson coefficients.', fontsize=12)\n",
    "ax1.tick_params(axis='x', labelsize=8)\n",
    "ax1.tick_params(axis='y', labelsize=8)\n",
    "\n",
    "# Create a mask for the upper triangle for the second correlation matrix\n",
    "mask_corr2 = np.triu(np.ones_like(data_clusters_outliers.corr(numeric_only=True), dtype=bool))\n",
    "\n",
    "# Draw the heatmap for the second correlation matrix\n",
    "sns.heatmap(data_clusters_outliers.corr(numeric_only=True), mask=mask_corr2, cmap=cmap,\n",
    "            vmax=1, vmin=-1, square=True, linewidths=.7, cbar_kws={\"shrink\": .8}, ax=ax2)\n",
    "ax2.set_title('Correlation Matrix (without outliers).\\nPearson coefficients.', fontsize=12)\n",
    "ax2.tick_params(axis='x', labelsize=8)\n",
    "ax2.tick_params(axis='y', labelsize=8)\n",
    "\n",
    "# Adjust the color bar for the second heatmap\n",
    "cbar2 = ax2.collections[0].colorbar\n",
    "cbar2.ax.tick_params(labelsize=8)\n",
    "\n",
    "# Adjust the layout of the figure to prevent overlapping of plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c99421",
   "metadata": {},
   "source": [
    "There is no big difference in the sense of correlation between the data with outliers and without them. It decresed a bit between features useless_parking and charging_duration_disconnect because we deleted some rare cases of long charging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a8b965",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_clusters_outliers.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f07e84c",
   "metadata": {},
   "source": [
    "There are some features with medium correlation for example charging_duration_disconnect and charging_duration_done (correlation=0.66), charging_duration_done and kWhDelivered\t(correlation=0.54). They are also important for clustering because describe quality of charging sessions. Correlation could give biases to the clustering results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67055b0",
   "metadata": {},
   "source": [
    "#### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b87132",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encoding categorical features\n",
    "data_new = data_clusters.copy()\n",
    "encoder = OrdinalEncoder()\n",
    "encoded_columns = encoder.fit_transform(data_new[['weather']])\n",
    "data_new[['weather']] = encoded_columns\n",
    "\n",
    "#check\n",
    "data_new.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5be2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for the data without outliers\n",
    "data_no_outliers = data_clusters_outliers.copy()\n",
    "encoded_columns = encoder.fit_transform(data_no_outliers[['weather']])\n",
    "data_no_outliers[['weather']] = encoded_columns\n",
    "\n",
    "data_no_outliers.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f53000",
   "metadata": {},
   "source": [
    "#### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cdefb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the scaler on the features and scale them\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data_new)\n",
    "X_scaled = scaler.transform(data_new)\n",
    "\n",
    "# Convert the scaled data back into a DataFrame\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=data_new.columns, index=data_new.index)\n",
    "\n",
    "# Display the first few rows of the scaled features\n",
    "print(X_scaled.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d54b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data without outliers\n",
    "scaler.fit(data_no_outliers)\n",
    "X_scaled_no_outliers = scaler.transform(data_no_outliers)\n",
    "\n",
    "# Convert the scaled data back into a DataFrame\n",
    "X_scaled_no_outliers = pd.DataFrame(X_scaled_no_outliers, columns=data_no_outliers.columns, index=data_no_outliers.index)\n",
    "\n",
    "# Display the first few rows of the scaled features\n",
    "print(X_scaled_no_outliers.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6371cce6",
   "metadata": {},
   "source": [
    "#### Data Dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e58fe97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data with outliers\n",
    "\n",
    "# Fit PCA on the scaled data\n",
    "pca = PCA().fit(X_scaled)\n",
    "\n",
    "# Create a figure \n",
    "plt.figure(figsize=(6, 3))  \n",
    "\n",
    "x = np.arange(1, len(pca.explained_variance_ratio_)+1, 1)\n",
    "\n",
    "# Plot the cumulative sum of the explained variance ratio to show \n",
    "# how much variance is explained by each additional component\n",
    "plt.plot(x, np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--', color='b')\n",
    "plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "plt.text(0.6, 0.87, '95% cut-off threshold', color = 'red', fontsize=8)\n",
    "plt.grid(axis='x')\n",
    "plt.title('PCA with outliers', fontsize=14)\n",
    "plt.xlabel('number of components', fontsize=8)\n",
    "plt.ylabel('cumulative explained variance', fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27648d0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform PCA on the scaled data specifying 10 components to keep\n",
    "pca = PCA(n_components=10).fit(X_scaled)\n",
    "\n",
    "# Transform the scaled data \n",
    "components = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Calculate the total explained variance in percentage\n",
    "total_var = pca.explained_variance_ratio_.sum() * 100\n",
    "\n",
    "# Get the number of components from the PCA model\n",
    "n_components = len(pca.explained_variance_ratio_)\n",
    "\n",
    "# Create labels for the scatter matrix plot, including the percentage of variance for each component\n",
    "labels = {\n",
    "    str(i): f\"PC {i+1} ({var:.1f}%)\"\n",
    "    for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n",
    "}\n",
    "\n",
    "# Create a scatter matrix of the PCA components \n",
    "fig = px.scatter_matrix(\n",
    "    components,\n",
    "    dimensions=range(n_components),\n",
    "    labels=labels,\n",
    "    title=f'Total Explained Variance: {total_var:.2f}%',\n",
    ")\n",
    "\n",
    "fig.update_traces(marker_size=1, diagonal_visible=False)\n",
    "\n",
    "fig.update_layout(\n",
    "    font=dict(size=8, color='black'),\n",
    "    autosize=False,\n",
    "    width=1100,\n",
    "    height=1100,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b573447d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10, random_state=RAND)\n",
    "X_embedding_pca_10 = pca.fit_transform(X_scaled)\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    X_embedding_pca_10, x=0, y=1, z=2,\n",
    "    labels={'color': 'species'}\n",
    ")\n",
    "fig.update_traces(marker_size=2)\n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a49accd",
   "metadata": {},
   "source": [
    "Visually, it's difficult to determine the clusters; the data points are densely grouped, and there are no natural gaps between the points.\n",
    "\n",
    "Let's do the same for the data without outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593257e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data without outliers\n",
    "pca_1 = PCA().fit(X_scaled_no_outliers)\n",
    "plt.figure(figsize=(6, 3))  \n",
    "x = np.arange(1, len(pca_1.explained_variance_ratio_)+1, 1)\n",
    "plt.plot(x, np.cumsum(pca_1.explained_variance_ratio_), marker='o', linestyle='--', color='b')\n",
    "plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "plt.text(0.6, 0.87, '95% cut-off threshold', color = 'red', fontsize=8)\n",
    "plt.grid(axis='x')\n",
    "plt.title('PCA without outliers', fontsize=14)\n",
    "plt.xlabel('number of components', fontsize=8)\n",
    "plt.ylabel('cumulative explained variance', fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99437b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_1 = PCA(n_components=10).fit(X_scaled_no_outliers)\n",
    "components = pca_1.fit_transform(X_scaled_no_outliers)\n",
    "total_var = pca_1.explained_variance_ratio_.sum() * 100\n",
    "n_components = len(pca_1.explained_variance_ratio_)\n",
    "\n",
    "labels = {\n",
    "    str(i): f\"PC {i+1} ({var:.1f}%)\"\n",
    "    for i, var in enumerate(pca_1.explained_variance_ratio_ * 100)\n",
    "}\n",
    "\n",
    "fig = px.scatter_matrix(\n",
    "    components,\n",
    "    dimensions=range(n_components),\n",
    "    labels=labels,\n",
    "    title=f'Total Explained Variance: {total_var:.2f}%',\n",
    ")\n",
    "\n",
    "fig.update_traces(marker_size=1, diagonal_visible=False)\n",
    "\n",
    "fig.update_layout(\n",
    "    font=dict(size=8, color='black'),\n",
    "    autosize=False,\n",
    "    width=1100,\n",
    "    height=1100,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d3027",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca_10_outliers = PCA(n_components=10, random_state=RAND)\n",
    "X_embedding_pca_10_outliers = pca_10_outliers.fit_transform(X_scaled_no_outliers)\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    X_embedding_pca_10_outliers, x=0, y=1, z=2,\n",
    "    labels={'color': 'species'}\n",
    ")\n",
    "fig.update_traces(marker_size=2)\n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47966fe9",
   "metadata": {},
   "source": [
    "On the first graph (with outliers) they can be seen as isolated points far from the dense areas, and they could affect the calculation of PCA components by pulling them in the direction of the outliers. On the second graph we also observe outliers, but in less quantity, and the data seems more compact and dense. This cleaner dataset can improve the performance of clustering algorithms by focusing on the more consistent, central patterns within the data. But the difference is not so big, so we check clustering algorythms on the both datasets to compare results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12d768a",
   "metadata": {},
   "source": [
    "#### K-Means Clustering Algorythm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4fe918",
   "metadata": {},
   "source": [
    "##### Determining the Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca17fda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We have dataset with 63000 data points, let's start from 200 clusters, we'll try to make as less as possible\n",
    "k_max = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3d0b21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a dictionary to store the clustering parameters for different algorithms.\n",
    "# For 'KMeans', we specify the 'random_state' parameter to ensure reproducibility of the results.\n",
    "params_cluster = {\n",
    "    'KMeans': {\n",
    "        'random_state': RAND,\n",
    "    },\n",
    "    'AgglomerativeClustering': {\n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21930ddd",
   "metadata": {},
   "source": [
    "Let's examine the data without outliers, because we are going to proceed with it. But we will also check this number of clusters on the data with outliers and compare quality of clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8e0624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data without outliers\n",
    "# counting inertia\n",
    "clusters_pca_1 = []\n",
    "losses_pca_1 = []\n",
    "  \n",
    "for k in range(k_max):\n",
    "    model = KMeans(n_clusters=k+1, n_init='auto')\n",
    "    model.fit(X_embedding_pca_10_outliers)\n",
    "    clusters_pca_1.append(k+1)\n",
    "    losses_pca_1.append(model.inertia_)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d00daad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying the 'elbow point' in the inertia plot\n",
    "plt.subplots(figsize=(6, 3))\n",
    "plt.plot(clusters_pca_1, losses_pca_1)\n",
    "plt.ylabel(\"Loss\", fontsize=8)\n",
    "plt.xlabel(\"Number of clusters\", fontsize=8)\n",
    "plt.title('Elbow method for the data without outliers', fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2436875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zooming the graph\n",
    "plt.subplots(figsize=(6, 3))\n",
    "plt.plot(clusters_pca_1, losses_pca_1)\n",
    "plt.ylabel(\"Loss\", fontsize=8)\n",
    "plt.xlabel(\"Number of clusters\", fontsize=8)\n",
    "plt.xlim([0,70]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdd9ab3",
   "metadata": {},
   "source": [
    "Based on this elbow plot, we could choose 5-10 clusters. But let's see also the silhoette score for 2-11 clusters for the final decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2918bc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_silhouette(df, a, b):\n",
    "    \"\"\"\n",
    "    Calculate and plot silhouette scores for different numbers of clusters to evaluate clustering performance.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: dataset to be clustered.\n",
    "    - a: starting number of clusters.\n",
    "    - b: ending number of clusters.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the number of rows and columns for the subplot grid \n",
    "    n_clusters_range = b - a\n",
    "    n_cols = 2  \n",
    "    # Calculate the number of rows needed\n",
    "    n_rows = (n_clusters_range + n_cols - 1) // n_cols  \n",
    "    # Initialize the subplot grid\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 7 * n_rows))  \n",
    "\n",
    "    # Ensure axes is always a 2D array to make a graph\n",
    "    axes = np.array(axes).reshape(n_rows, n_cols)\n",
    "\n",
    "    # Perform clustering and calculate silhouette scores\n",
    "    for idx, n_clusters in enumerate(range(a, b)):\n",
    "        ax = axes[idx // n_cols, idx % n_cols]  \n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=RAND, n_init=10)  \n",
    "        cluster_labels = kmeans.fit_predict(df)\n",
    "        silhouette_avg = silhouette_score(df, cluster_labels)   \n",
    "        # Calculate the silhouette score for each sample\n",
    "        sample_silhouette_values = silhouette_samples(df, cluster_labels)  \n",
    "\n",
    "        y_lower = 10  \n",
    "        # Plot the silhouette scores of each sample\n",
    "        for i in range(n_clusters):\n",
    "            ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]  \n",
    "            ith_cluster_silhouette_values.sort()  # Sort the scores for better visualization\n",
    "\n",
    "            size_cluster_i = ith_cluster_silhouette_values.shape[0]  # Get the number of samples in cluster i\n",
    "            y_upper = y_lower + size_cluster_i  # Calculate the upper bound for the y-axis\n",
    "            color = plt.cm.nipy_spectral(float(i) / n_clusters)  # Assign a color to the cluster\n",
    "\n",
    "            # Fill the area under the silhouette curve\n",
    "            ax.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color, \n",
    "                             edgecolor=color, alpha=0.7)\n",
    "\n",
    "            # Label the silhouette plots \n",
    "            ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i), fontsize=12)\n",
    "\n",
    "            y_lower = y_upper + 10  \n",
    "\n",
    "        # Set the title, xlabel, and ylabel for the subplot\n",
    "        ax.set_title(f\"Silhouette Plot for n_clusters = {n_clusters}\", fontsize=16)\n",
    "        ax.set_xlabel(\"Silhouette Coefficient Value\", fontsize=14)\n",
    "        ax.set_ylabel(\"Cluster Number\", fontsize=14)\n",
    "\n",
    "        # Draw a vertical line to show the average silhouette score across all samples\n",
    "        ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "        # Annotate the average silhouette score\n",
    "        ax.text(silhouette_avg + 0.02, y_lower + 5, f\"Silhouette = {silhouette_avg:.2f}\", color=\"red\", fontsize=12)\n",
    "\n",
    "    # Remove any empty subplots \n",
    "    for i in range(idx + 1, len(axes.flat)):\n",
    "        fig.delaxes(axes.flat[i])\n",
    "\n",
    "    # Adjust the layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d17eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cluster_silhouette(X_embedding_pca_10_outliers, 2, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0b0371",
   "metadata": {},
   "source": [
    "With 2-6 clusters we observe the score 0.15, after it increse to 0.16, but it will be too many clusters that isn't matched with the idea of explainability, so let's think about dividing the data to 2-6 clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df7ab97",
   "metadata": {},
   "source": [
    "##### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42c021a",
   "metadata": {},
   "source": [
    "Firstly, lets's see at the cluster's sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6884bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_size(data, labels, ax):\n",
    "    \"\"\"\n",
    "    Plot the size of each cluster\n",
    "\n",
    "    Parameters:\n",
    "    data: the original dataset\n",
    "    labels: cluster labels for each data point\n",
    "    ax : matplotlib axes object to draw the plot on\n",
    "    \"\"\"\n",
    "    # Adding cluster labels to the data\n",
    "    cluster_data = data.assign(cluster=labels)\n",
    "\n",
    "    # Counting the number of data points in each cluster\n",
    "    cluster_counts = cluster_data['cluster'].value_counts().sort_index()\n",
    "\n",
    "    # Creating a bar plot\n",
    "    sns.barplot(x=cluster_counts.index, y=cluster_counts.values, ax=ax)\n",
    "    ax.set_xlabel('Cluster Label')\n",
    "    ax.set_ylabel('Number of Data Points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da027bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_clustering_pca(data, \n",
    "                        data_scale, \n",
    "                        embedding, \n",
    "                        model: sklearn.base.ClusterMixin,\n",
    "                        kwargs, \n",
    "                        min_size: int = 2, \n",
    "                        max_size: int = 6, \n",
    "                        type_train: str = 'embedding'):\n",
    "    \"\"\"\n",
    "    Visualize cluster sizes using PCA-reduced data \n",
    "    \n",
    "    Parameters:\n",
    "    - data: the original dataframe for reference in plotting\n",
    "    - data_scale: scaled data\n",
    "    - embedding: PCA-reduced data \n",
    "    - model: clustering model from scikit-learn that implements ClusterMixin\n",
    "    - kwargs: additional keyword arguments for the clustering model.\n",
    "    - min_size: minimum number of clusters \n",
    "    - max_size: maximum number of clusters \n",
    "    - type_train: indicates whether to fit the model on 'data_scale' or 'embedding'.\n",
    "    \n",
    "    Returns:\n",
    "    - dict_clusters: a dictionary with the number of clusters as keys and cluster labels as values.\n",
    "    \"\"\"\n",
    "    \n",
    "    dict_clusters = {}\n",
    "    fig, axes = plt.subplots(1, max_size - min_size + 1, figsize=(16, 4))\n",
    "\n",
    "    for i, clust in enumerate(range(min_size, max_size + 1)):\n",
    "        # Ensure 'n_init' is explicitly set for KMeans models\n",
    "        if 'KMeans' in str(model):\n",
    "            n_init = kwargs.pop('n_init', 'auto')  \n",
    "            clf = model(n_clusters=clust, n_init=n_init, **kwargs)\n",
    "        else:\n",
    "            clf = model(n_clusters=clust, **kwargs)\n",
    "\n",
    "        # Fitting the model on the chosen data type\n",
    "        if type_train == 'embedding':\n",
    "            clf.fit(embedding)\n",
    "        else:\n",
    "            clf.fit(data_scale)\n",
    "\n",
    "        # Saving the cluster labels in the dictionary\n",
    "        dict_clusters[clust] = clf.labels_\n",
    "\n",
    "        # Plotting the size of each cluster using the plot_size function written before\n",
    "        ax = axes[i] if max_size - min_size > 0 else axes\n",
    "        plot_size(data, dict_clusters[clust], ax=ax)\n",
    "\n",
    "    # Setting the main title and adjust layout to prevent overlap of subplots\n",
    "    plt.suptitle('Cluster Sizes for Different Number of Clusters', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "    plt.show()\n",
    "\n",
    "    return dict_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeffab7e-7d23-44b1-8357-696f66999dfe",
   "metadata": {},
   "source": [
    "Firsty we will have a look at the data with outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08adb6eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data with outliers\n",
    "dict_clusters_pca = plot_clustering_pca(\n",
    "    data=data_new,\n",
    "    data_scale=X_scaled,\n",
    "    embedding=X_embedding_pca_10,\n",
    "    model=KMeans,\n",
    "    kwargs=params_cluster['KMeans'],\n",
    "    type_train='embedding'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e32973-002a-4609-abc8-0ef0f01f848c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_done_1 = data_new.copy()\n",
    "\n",
    "for n_clusters in range(2, 7):  \n",
    "    data_done_1 = data_done_1.assign(cluster_km=dict_clusters_pca[n_clusters])\n",
    "    \n",
    "    print(f'silhouette K-Means for {n_clusters} clusters: {round(silhouette_score(data_done_1, data_done_1.cluster_km), 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9396ce9-0b66-49b2-8563-6646dbbe2439",
   "metadata": {},
   "source": [
    "Now we will do the same for the data without outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a80bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data without outliers\n",
    "dict_clusters_pca_outliers = plot_clustering_pca(\n",
    "    data=data_no_outliers,\n",
    "    data_scale=X_scaled_no_outliers,\n",
    "    embedding=X_embedding_pca_10_outliers,\n",
    "    model=KMeans,\n",
    "    kwargs=params_cluster['KMeans'],\n",
    "    type_train='embedding'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7650a6-239d-488f-afb3-081f43ae6316",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_done = data_no_outliers.copy()\n",
    "\n",
    "for n_clusters in range(2, 7):  \n",
    "    data_done = data_done.assign(cluster_km=dict_clusters_pca_outliers[n_clusters])\n",
    "    \n",
    "    print(f'silhouette K-Means for {n_clusters} clusters: {round(silhouette_score(data_done, data_done.cluster_km), 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031cd832",
   "metadata": {},
   "source": [
    "As we can see, the data with outliers and without them is divided a bit differently, we could observe different cluster sizes and maybe more clear patterns. The silhoette score for the data without outliers is much higher, so we consider to observe clustering on the dataset without outliers. According to the highest silhouette score let's see divide to 2-4 clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0facb7ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assigning columns to describe in analysis\n",
    "object_cols = [\n",
    "    'weather'\n",
    "    \n",
    "]\n",
    "\n",
    "num_cols = [\n",
    "    'siteID',\n",
    "    'kWhDelivered',\n",
    "    'temperature',\n",
    "    'pressure',\n",
    "    'charging_duration_disconnect',\n",
    "    'charging_duration_done',\n",
    "    'useless_parking',\n",
    "    'connection_year', \n",
    "    'connection_month', \n",
    "    'connection_hour', \n",
    "    'disconnect_time', \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950b8ad4",
   "metadata": {},
   "source": [
    "We write 3 functions which help us to analyse the clustering results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc9fc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_object(data, labels, object_cols):\n",
    "    \"\"\"\n",
    "    Plot categorical features for different clusters, normalized by the size of each cluster.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame containing the data.\n",
    "    - labels: Series or array-like containing cluster labels for each row in `data`.\n",
    "    - object_cols: List of column names in `data` that are categorical and should be plotted.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure labels are of string type to prevent issues with seaborn plotting\n",
    "    labels = labels.astype(str)\n",
    "\n",
    "    # Combine the dataset with cluster labels\n",
    "    data_label = data.assign(cluster=labels)\n",
    "\n",
    "    # Count the number of entries in each cluster\n",
    "    size_cluster = data_label.groupby(\"cluster\").size()\n",
    "\n",
    "    # Determine the number of rows needed for subplots based on the number of categorical columns\n",
    "    rows = len(object_cols)\n",
    "    n_cols = 1  # Fixed number of columns for subplot grid\n",
    "\n",
    "    # Create a subplot for each categorical column\n",
    "    fig, axes = plt.subplots(nrows=rows, ncols=n_cols, figsize=(10, 6 * rows))\n",
    "\n",
    "    # Ensure `axes` is an array even when there's only one subplot to maintain consistency\n",
    "    if rows == 1:\n",
    "        axes = np.array([axes])\n",
    "\n",
    "    for num, col in enumerate(object_cols):\n",
    "        # Calculate the normalized count for each category within each cluster\n",
    "        data_norm = (\n",
    "            data_label.groupby([\"cluster\", col]).size()\n",
    "            / size_cluster\n",
    "        ).reset_index(name='norm')\n",
    "        \n",
    "        # Sort categories by their overall frequency for better visualization\n",
    "        order = data_norm.groupby(col)['norm'].sum().sort_values(ascending=False).index\n",
    "\n",
    "        # Plot the normalized counts as bar plots\n",
    "        sns.barplot(\n",
    "            data=data_norm,\n",
    "            y=col,  \n",
    "            x='norm',  \n",
    "            hue=\"cluster\",  \n",
    "            ax=axes[num],  \n",
    "            order=order \n",
    "        )\n",
    "\n",
    "        axes[num].set_xlabel('Normalized Count')\n",
    "        axes[num].set_ylabel(col)\n",
    "        axes[num].tick_params(axis='y', labelsize=10)\n",
    "        axes[num].legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        axes[num].set_title(f'Distribution of {col} by Cluster')\n",
    "\n",
    "    # Adjust subplot layout to prevent overlapping and display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908babd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plotting_kde_num(data, labels, num_cols):\n",
    "    \"\"\"\n",
    "    Make Kernel Density Estimate plot for comparing numerical features \n",
    "    data: dataset\n",
    "    labels: cluster labels\n",
    "    num_cols: list of columns wuth numerical features\n",
    "    \"\"\"\n",
    "    data_label = data.assign(cluster=labels)\n",
    "\n",
    "    # Set the number of columns and rows for subplots\n",
    "    n_cols = 3  \n",
    "    n_rows = (len(num_cols) + n_cols - 1) // n_cols  # Calculate the necessary number of rows\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(10, 4 * n_rows))\n",
    "    axes = axes.flatten()  # Flatten to a one-dimensional array for convenience\n",
    "\n",
    "    # Draw the plots\n",
    "    for idx, col in enumerate(num_cols):\n",
    "        if data_label[col].var() == 0:  # Check for zero variance\n",
    "            axes[idx].text(0.5, 0.5, 'No variance in\\n' + col, \n",
    "                           horizontalalignment='center', \n",
    "                           verticalalignment='center', \n",
    "                           transform=axes[idx].transAxes)\n",
    "            axes[idx].set_title(col)\n",
    "            axes[idx].set(xlabel='', ylabel='', xticklabels=[], yticklabels=[])\n",
    "            continue  # Skip the rest of the loop and proceed with the next iteration\n",
    "\n",
    "        sns.kdeplot(\n",
    "            data=data_label,\n",
    "            x=col,\n",
    "            hue=\"cluster\",\n",
    "            fill=True,\n",
    "            common_norm=False,\n",
    "            palette=\"crest\",\n",
    "            alpha=0.4,\n",
    "            linewidth=2,\n",
    "            ax=axes[idx],\n",
    "        )\n",
    "\n",
    "    # Remove any empty subplots\n",
    "    for i in range(len(num_cols), len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329a3ec4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plotting_num(data, labels, num_cols):\n",
    "    \"\"\"\n",
    "    Make boxplot for comparing numerical features \n",
    "    data: dataset\n",
    "    labels: cluster labels\n",
    "    num_cols: list of columns wuth numerical features\n",
    "    \"\"\"\n",
    "    data_label = data.assign(cluster=labels)\n",
    "\n",
    "    n_cols = 3  \n",
    "    n_rows = (len(num_cols) + n_cols - 1) // n_cols  \n",
    "\n",
    "    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(10, 5 * n_rows))\n",
    "    axes = axes.flatten()  \n",
    "   \n",
    "    for idx, col in enumerate(num_cols):\n",
    "        if data_label[col].var() == 0: \n",
    "            axes[idx].text(0.5, 0.5, 'No variance in\\n' + col, \n",
    "                           horizontalalignment='center', \n",
    "                           verticalalignment='center', \n",
    "                           transform=axes[idx].transAxes)\n",
    "            axes[idx].set_title(col)\n",
    "            axes[idx].set(xlabel='', ylabel='', xticklabels=[], yticklabels=[])\n",
    "            continue  \n",
    "        \n",
    "        sns.boxplot(\n",
    "            data=data_label,\n",
    "            y=col,\n",
    "            x=\"cluster\",\n",
    "            palette=\"crest\",\n",
    "            ax=axes[idx],\n",
    "        )\n",
    "\n",
    "    for i in range(len(num_cols), len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f58e68-f643-4461-82bc-3d8115b27b9e",
   "metadata": {},
   "source": [
    "Now we are ready to observe the clusters content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90781dbf-6645-42df-b49e-1807796eba93",
   "metadata": {},
   "source": [
    "##### 2 clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32335c9f-3b9a-435f-9b75-a18a84acbdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_object(data_clusters_outliers, dict_clusters_pca_outliers[2], object_cols)\n",
    "plotting_kde_num(data_clusters_outliers, dict_clusters_pca_outliers[2], num_cols)\n",
    "plotting_num(data_clusters_outliers, dict_clusters_pca_outliers[2], num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516d184e-4b14-4c0c-85d7-66baaf4bce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(\n",
    "    X_embedding_pca_10_outliers, x=0, y=1, z=2,\n",
    "    labels={'color': 'species'},\n",
    "    color=dict_clusters_pca_outliers[2]\n",
    ")\n",
    "fig.update_traces(marker_size=2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5376dacb-0fa6-4afd-93c7-a8c4869d35c7",
   "metadata": {},
   "source": [
    "**Cluster 0: Short Public Sessions**\n",
    "\n",
    "Mostly site 2 (public), with medium or short kWh delivered (0-15). Parking without charging tends to be short, and the connection time is quite stable during the day (from 8 am to 3 pm), with disconnections mostly occurring around 6-7 pm. **People who use stations in public places occasionally for short top-ups.**\n",
    "\n",
    "**Cluster 1: Long Private Sessions**\n",
    "\n",
    "Primarily private sites (site 1), with medium to high energy delivered (20-25 kWh). Charging sessions and parking without charging are long (more than 4 hours). Connections occur very early (from 7 am to 10 am); after 10 am, new connections almost don't happen. Disconnections occur between 5 pm and 7 pm. **People who use charging stations close to work and charge from early morning throughout the working day.**\n",
    "\n",
    "Value for business: helps in planning and optimizing charging infrastructure by ensuring that public stations are equipped for quick charges and are strategically located and private ones support longer charging durations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a17482",
   "metadata": {},
   "source": [
    "##### 3 clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6036f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_object(data_clusters_outliers, dict_clusters_pca_outliers[3], object_cols)\n",
    "plotting_kde_num(data_clusters_outliers, dict_clusters_pca_outliers[3], num_cols)\n",
    "plotting_num(data_clusters_outliers, dict_clusters_pca_outliers[3], num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f02803",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(\n",
    "    X_embedding_pca_10_outliers, x=0, y=1, z=2,\n",
    "    labels={'color': 'species'},\n",
    "    color=dict_clusters_pca_outliers[3]\n",
    ")\n",
    "fig.update_traces(marker_size=2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d47800a-cb78-4e3a-8852-a38c7a8f8bb8",
   "metadata": {},
   "source": [
    "**Cluster 0: Flexible Working Chargers**\n",
    "\n",
    "Site 1 (private), with a medium to large amount of energy delivered (10-20 kWh), and a wide range of session durations. There is a significant demand for connections at 8 am, but demand is lower throughout the day, with disconnections at 6 pm. **Connections are made close to the office during work hours to meet various energy demands.**\n",
    "\n",
    "Value for business: usefull to plan tailored services such as dynamic pricing, reservation systems, or loyalty programs which could attract this user base. Also could plan some marketing targeted services to optimize utilization and increase revenue from these users.\n",
    "\n",
    "**Cluster 1: First Working Users**\n",
    "\n",
    "Site 1 (private), with a medium to large amount of energy delivered, featuring the longest session durations and parking times. More active in the early years of observation (2018-19). High demand in the morning (7-9 am), with disconnections between 4-6 pm. **First clients of the servise. Connections are made close to the office for the entire day, without time limitations.**\n",
    "\n",
    "Value for business: efficient energy management and the opportunity to offer tailored energy packages or subscription models. Opportunities for off-peak energy management strategies to optimize grid load and reduce costs.\n",
    "\n",
    "**Cluster 2: Public Chargers**\n",
    "\n",
    "Mostly site 2 (public), with short to medium sessions and a medium amount of energy delivered. Parking duration is also short. Demand from 9 am to 4 pm, with disconnections mostly around 6-7 pm. **Connections in public places are for short durations for a quick energy boost**\n",
    "\n",
    "Value for business: strategic placement of fast-charging stations in high-traffic public areas, mobile app integration for real-time charger availability and easy payment options. **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c921439",
   "metadata": {},
   "source": [
    "##### 4 clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cdbcb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotting_object(data_clusters_outliers, dict_clusters_pca_outliers[4], object_cols)\n",
    "plotting_kde_num(data_clusters_outliers, dict_clusters_pca_outliers[4], num_cols)\n",
    "plotting_num(data_clusters_outliers, dict_clusters_pca_outliers[4], num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe732ead",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(\n",
    "    X_embedding_pca_10_outliers, x=0, y=1, z=2,\n",
    "    labels={'color': 'species'},\n",
    "    color=dict_clusters_pca_outliers[4]\n",
    ")\n",
    "fig.update_traces(marker_size=2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97917419-6b85-4a59-a657-8a383b6ced5c",
   "metadata": {},
   "source": [
    "**Cluster 0: New Regular Consumers**\n",
    "\n",
    "Exhibits three peaks for kWh delivered, around 5, 10, and 20 kWh. Charging duration is among the lowest, with mostly short parking without charging, but some long sessions are also present. Active from 2019-2021. Connections typically occur around 8 am, with disconnections at either 11 am or 7 pm. Charging occurs at both sites, but predominantly at Site 1 (private).  **Users charge close to work for long sessions, possibly to fully charge their batteries before going home.**\n",
    "\n",
    "Value for business: time-based incentives and flexible pricing models, rewards for frequent use could encourage off-peak charging, helping to balance the grid's demand and increase station utilization during lower-demand periods. \n",
    "\n",
    "**Cluster 1: First and Flexible**\n",
    "\n",
    "Characterized by low kWh delivery, mostly under 10 kWh, with occasional peaks around 20 kWh. Charging durations are among the longest, with the longest parking durations without charging (around 4 hours). Most active in the initial years, 2018-2019. Connections vary from 6 to 10 am, predominantly at 7 am, with disconnections between 5-6 pm, across both private and public sites. **Early adopters of the service, they prefer to fully charge, possibly at stations close to their work, homes, or places where they stay for extended periods**\n",
    "\n",
    "Value for business: targeted promotions, such as loyalty rewards or discounts for consistent usage can enhance usage frequency and solidify their loyalty. Personalized communication highlighting new features or stations could further encourage engagement.\n",
    "\n",
    "**Cluster 2: Charging Professionals**\n",
    "\n",
    "Prefers charging at Site 1 (private), with high kWh deliveries (over 20), indicating either larger battery capacities or less frequent charging needs. Features the longest session durations, but short parking without charging times. Initially most active in 2018-2019, but also present in 2020-2021. Connections typically occur between 8-9 am, with disconnections from 5-6 pm.  **Professionals who rely on charging stations near their workplaces.** \n",
    "\n",
    "Value for business: tailored B2B solutions and partnerships, providing dedicated charging infrastructure or reserved charging slots for employees. Off-peak pricing models for long-term parking.\n",
    "\n",
    "**Cluster 3: First Fast Public Users**\n",
    "\n",
    "Primarily uses Site 2 (public), active in 2018-2019, with the shortest kWh deliveries and charging sessions, and very short parking times without charging. Connections occur at various times, notably at 9 am and 2-3 pm, with disconnections broadly spread but mostly around 6 pm. **Occasional daily users with diverse charging needs and routines.**\n",
    "\n",
    "Value for business: targeted marketing with commercial fleets that could benefit from fast, public charging. Priority services, reserved spots during peak hours, or discounted rates for frequent users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ab08c1",
   "metadata": {},
   "source": [
    "### AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dbab81-ec0c-4a8a-b2b7-ad6f03c64104",
   "metadata": {},
   "source": [
    "To determined the number of clusters let's have a look at the silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a0d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt the function which we used to plot the score for Agglomerative clustering\n",
    "def cluster_silhouette_agglo(df, a, b):\n",
    "    # Define the number of rows and columns for subplots\n",
    "    n_clusters_range = b - a\n",
    "    n_cols = 2\n",
    "    n_rows = (n_clusters_range + n_cols - 1) // n_cols  # Calculate the number of rows\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 7 * n_rows))\n",
    "\n",
    "    # Ensure axes is always a 2D array\n",
    "    if n_rows == 1:\n",
    "        axes = np.array(axes).reshape(-1, n_cols)\n",
    "    elif n_cols == 1:\n",
    "        axes = np.array(axes).reshape(n_rows, -1)\n",
    "\n",
    "    for idx, n_clusters in enumerate(range(a, b)):\n",
    "        ax = axes[idx // n_cols, idx % n_cols]\n",
    "        model = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "        cluster_labels = model.fit_predict(df)\n",
    "\n",
    "        silhouette_avg = silhouette_score(df, cluster_labels)\n",
    "        sample_silhouette_values = silhouette_samples(df, cluster_labels)\n",
    "\n",
    "        y_lower = 10\n",
    "        for i in range(n_clusters):\n",
    "            ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "            ith_cluster_silhouette_values.sort()\n",
    "\n",
    "            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "            y_upper = y_lower + size_cluster_i\n",
    "            color = plt.cm.nipy_spectral(float(i) / n_clusters)\n",
    "            ax.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "            ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i), fontsize=12)\n",
    "\n",
    "            y_lower = y_upper + 10\n",
    "\n",
    "        ax.set_title(f\"Silhouette Plot for n_clusters = {n_clusters}\", fontsize=16)\n",
    "        ax.set_xlabel(\"Silhouette Coefficient Value\", fontsize=14)\n",
    "        ax.set_ylabel(\"Cluster Number\", fontsize=14)\n",
    "        ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "        ax.text(silhouette_avg + 0.02, y_lower + 5, f\"Silhouette = {silhouette_avg:.2f}\", color=\"red\", fontsize=12)\n",
    "\n",
    "    # Remove any empty subplots\n",
    "    for i in range(idx + 1, len(axes.flat)):\n",
    "        fig.delaxes(axes.flat[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64a2810-6344-41a4-bca8-349d597aa563",
   "metadata": {},
   "source": [
    "For explainability purposes, we prefer to limit the number of clusters to six. The final cluster count will be determined based on silhouette scores. Additionally, due to the higher computational demands of the agglomerative method, we aim to conserve resources by avoiding the creation of an excessive number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadce4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cluster_silhouette_agglo(X_embedding_pca_10_outliers, 2, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e8724f",
   "metadata": {},
   "source": [
    "The highest score we could reach with 5-6 clusters (0.12), so let's divide like this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092cb2f1-c914-40be-8aab-23a7bc414e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajusted function fo agglomarative clustering\n",
    "def plot_clustering_agglo(data, \n",
    "                          data_scale, \n",
    "                          embedding, \n",
    "                          kwargs,\n",
    "                          min_size=5, \n",
    "                          max_size=6, \n",
    "                          type_train='embedding'):\n",
    "    \"\"\"\n",
    "    Visualize cluster sizes using PCA-reduced data with Agglomerative Clustering.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: the original dataframe for reference in plotting.\n",
    "    - data_scale: scaled data.\n",
    "    - embedding: PCA-reduced data.\n",
    "    - kwargs: additional keyword arguments for the clustering model.\n",
    "    - min_size: minimum number of clusters.\n",
    "    - max_size: maximum number of clusters.\n",
    "    - type_train: indicates whether to fit the model on 'data_scale' or 'embedding'.\n",
    "    \n",
    "    Returns:\n",
    "    - dict_clusters: a dictionary with the number of clusters as keys and cluster labels as values.\n",
    "    \"\"\"\n",
    "    \n",
    "    dict_clusters = {}\n",
    "    fig, axes = plt.subplots(1, max_size - min_size + 1, figsize=(16, 4))\n",
    "    \n",
    "    for i, clust in enumerate(range(min_size, max_size + 1)):\n",
    "        # Initialize the AgglomerativeClustering model\n",
    "        clf = AgglomerativeClustering(n_clusters=clust, **kwargs)\n",
    "        \n",
    "        # Fit the model on the chosen data type\n",
    "        labels = clf.fit_predict(embedding if type_train == 'embedding' else data_scale)\n",
    "        \n",
    "        # Save the cluster labels in the dictionary\n",
    "        dict_clusters[clust] = labels\n",
    "\n",
    "        # Plotting the size of each cluster\n",
    "        ax = axes[i] if max_size - min_size > 0 else axes\n",
    "        plot_size(data, dict_clusters[clust], ax=ax)\n",
    "    \n",
    "    # Setting the main title and adjust layout to prevent overlap of subplots\n",
    "    plt.suptitle('Cluster Sizes for Different Number of Clusters', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "    plt.show()\n",
    "    \n",
    "    return dict_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240aa419-6c71-4e46-b42e-e2037553a9f1",
   "metadata": {},
   "source": [
    "Let's see the cluster sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6ba219",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dict_clusters_agglo = plot_clustering_agglo(\n",
    "    data=data_no_outliers,\n",
    "    data_scale=X_scaled_no_outliers,\n",
    "    embedding=X_embedding_pca_10_outliers,\n",
    "    kwargs=params_cluster['AgglomerativeClustering'],\n",
    "    type_train='embedding'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44646b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "\n",
    "    # Children of hierarchical clustering\n",
    "    children = model.children_\n",
    "\n",
    "    # Distances between each pair of children\n",
    "    # Since we don't have this information, we can use a uniform one for plotting\n",
    "    distance = np.arange(children.shape[0])\n",
    "\n",
    "    # The number of observations contained in each cluster level\n",
    "    no_of_observations = np.arange(2, children.shape[0]+2)\n",
    "\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "    linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489431f",
   "metadata": {},
   "source": [
    "##### 5 clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d66154",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "agglo_5 = AgglomerativeClustering(n_clusters=5) \n",
    "pred_agglo_5 = agglo_5.fit_predict(X_embedding_pca_10_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbc6598",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Dendrogram\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.title('Hierarchical Clustering Dendrogram for 5 Clusters')\n",
    "plot_dendrogram(agglo_5, labels=dict_clusters_agglo[5])\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8254070",
   "metadata": {},
   "source": [
    "The dendrogram seems quite complex. To see the connections between data points we will make another truncated dendrogram to focus on the most significant cluster mergers in hierarchical clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9010af0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the ward method which minimizes the variance of the clusters being merged\n",
    "Z = linkage(X_embedding_pca_10_outliers, 'ward')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title('Hierarchical Clustering Dendrogram for 5 Clusters')\n",
    "\n",
    "# Determine the color threshold to differentiate clusters\n",
    "color_threshold = Z[-5, 2] - 0.01  # Subtracting a small value to ensure the threshold is just below the merge point.\n",
    "\n",
    "# Generate the dendrogram plot.\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',    # 'lastp' truncation mode shows only the last 'p' merged clusters.\n",
    "    p=5,                     \n",
    "    leaf_rotation=90.,        # Rotate the leaf labels (x-axis labels) to avoid overlap and improve readability.\n",
    "    leaf_font_size=8.,        \n",
    "    show_contracted=True,     # Enable a condensed view to provide an overview of the larger clusters' structure.\n",
    "    color_threshold=color_threshold  # Apply the calculated color threshold to distinguish clusters by color.\n",
    ")\n",
    "\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c06e87",
   "metadata": {},
   "source": [
    "Now we wil see the content of the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aaf506",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_object(data_clusters_outliers, dict_clusters_agglo[5], object_cols)\n",
    "plotting_kde_num(data_clusters_outliers, dict_clusters_agglo[5], num_cols)\n",
    "plotting_num(data_clusters_outliers, dict_clusters_agglo[5], num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66cc280",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(\n",
    "    X_embedding_pca_10_outliers, x=0, y=1, z=2,\n",
    "    labels={'color': 'species'},\n",
    "    color=dict_clusters_agglo[5]\n",
    ")\n",
    "fig.update_traces(marker_size=2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8660a2d-6bb1-4b00-bc91-03352e95f69b",
   "metadata": {},
   "source": [
    "**Cluster 0: First Working Clients** \n",
    "\n",
    "Mostly fair weather, private and public places in 2018-19. Highest kWh delivered (25 and 40), longest charging duration, but using station without charging is short. Connection through all day but mostly at 8 am, disconnection at 6 pm, but could be also at night. \n",
    "\n",
    "Value for business: subscription-based plans for guaranteed access to charging spots, especially during peak hours. Providing workplace charging or partnering with employers could ensure loyalty and regular usage.\n",
    "\n",
    "**Cluster 1: First Public Occasional Clients** \n",
    "\n",
    "Site 2 (public) in 2018-1019, lowest kWh delivered (up to 10), lowest session's duration and parking time, connection time spread throught all day with light peaks for 9 am, 3-4 pm, disconnection at 7 pm. \n",
    "\n",
    "Value for business: engagement methods like discounted rates during off-peak hours or loyalty points for each session.\n",
    "\n",
    "**Cluster 2: Late Workers** \n",
    "\n",
    "Variance of cloudy, rain, smoke. Mostly site 1 (private) in 2018-19, medium kWh delivered, connection since 11-12 am till 5 pm, there are some cases with parking without charging.\n",
    "\n",
    "Value for business: offerings like late charging discounts or reserved spots, increasing station utilization during off-peak hours.\n",
    "\n",
    "**Cluster 3: Modern Working Clients** \n",
    "\n",
    "Highest occurence of the fair weather. Site 1 (private), medium kWh delivered, active mostly in 2021, charging fromm 8 am till 5-6pm. \n",
    "\n",
    "Value for business: services that align with a regular work schedule, such as reserved parking with charging during work hours. \n",
    "\n",
    "**Cluster 4: Full Chargers** \n",
    "\n",
    "Charging at both sites, medium and high energy delivered (10-20 kWh), longest parking without charging (4 hours), earliest sessions from 5 am till 9 am, disconnection around 5-6 pm. \n",
    "\n",
    "Value for business: priority services, such as dedicated charging areas and faster chargers, can ensure that these high-usage clients maintain a high level of satisfaction. Dynamic pricing structure that rewards higher consumption could also incentivize continued loyalty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a39e9dd",
   "metadata": {},
   "source": [
    "##### 6 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400e76c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "agglo_6 = AgglomerativeClustering(n_clusters=6) #The number of clusters to find\n",
    "pred_agglo_6 = agglo_6.fit_predict(X_embedding_pca_10_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1c0921",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Dendrogram\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.title('Hierarchical Clustering Dendrogram for 6 Clusters')\n",
    "plot_dendrogram(agglo_6, labels=dict_clusters_agglo[5])\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09df4ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Truncated dendrogram\n",
    "Z = linkage(X_embedding_pca_10_outliers, 'ward')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title('Hierarchical Clustering Dendrogram for 6 Clusters')\n",
    "\n",
    "color_threshold = Z[-5, 2] - 0.01  # Subtracting a small value to ensure the threshold is just below the merge point.\n",
    "\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',   \n",
    "    p=6,                     \n",
    "    leaf_rotation=90.,        \n",
    "    leaf_font_size=8.,        \n",
    "    show_contracted=True,      \n",
    "    color_threshold=color_threshold  \n",
    ")\n",
    "\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc5a5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_object(data_clusters_outliers, dict_clusters_agglo[6], object_cols)\n",
    "plotting_kde_num(data_clusters_outliers, dict_clusters_agglo[6], num_cols)\n",
    "plotting_num(data_clusters_outliers, dict_clusters_agglo[6], num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a39a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(\n",
    "    X_embedding_pca_10_outliers, x=0, y=1, z=2,\n",
    "    labels={'color': 'species'},\n",
    "    color=dict_clusters_agglo[6]\n",
    ")\n",
    "fig.update_traces(marker_size=2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c6a949-3794-4e83-a253-f37d06b82ac0",
   "metadata": {},
   "source": [
    "**Cluster 0: First Stable Clients** \n",
    "\n",
    "Mostly fair weather. Private and public places. High range of the delivered energy (0-20 kWh), but the shortest charging and parking duration. Active in 2018-19. Stable connection demand since 8 am till 5 pm, disconnection at 7 pm. **Mid-duration charge, behavior fits within a standard workday**\n",
    "\n",
    "**Cluster 1: Long Day Parkers** \n",
    "\n",
    "Medium energy delivered, medium charging duration, but longest parking without charging (more than 4 hours). Connection 5-6 am, disconnection 4-8 pm. **They leave a car in the morning for all day not chacking the charging session, park for longer than it needed**\n",
    "\n",
    "**Cluster 2: Fast Day Chargers** \n",
    "\n",
    "Most different weather (variance of \"cloudy\", smoke, storm). Medium energy delivered, shortest parking duration, connection around 11 am. **Prefer to make fast boost of energy in the morning**\n",
    "\n",
    "**Cluster 3: Top-up Close To Office**\n",
    "\n",
    "Highest occurence of the fair weather. Medium energy delivered (around 20 kWh), occurs mostly in 2021, private place (1), connection and disconnection around 11 am, possibly very short charging. **Charging close to job but not typically long**\n",
    "\n",
    "**Cluster 4: Long Chargers** \n",
    "\n",
    "Highest energy delivered (around 40 kWh), longest charging sessions. **Those who prefer full charge or prepare for a long day driving**\n",
    "\n",
    "**Cluster 5: Night Public Chargers** \n",
    "\n",
    "Mostly site 2 (public), medium energy consumption (25 kWh), stable long parking without charging (could be 6 hours). Connection hour around 9 pm, disconnection at 1 am or 7 am. **Those who leave a car for a several hours at night at the public place**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248afc56",
   "metadata": {},
   "source": [
    "#### Conclusion of the Cluster Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c174392b",
   "metadata": {},
   "source": [
    "Based on the cluster descriptions and silhouette score, for K-means clustering with 4 clusters appears to be more explanatory. For Agglomerative clustering we prefered 5 clusters result. Let's compare the results of both algorythms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315f259c-d07c-451d-92df-3acfffb67e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding labels of the best clustering results\n",
    "data_fin = data_no_outliers.copy()\n",
    "data_fin = data_fin.assign(cluster_km=dict_clusters_pca_outliers[4], \n",
    "                           cluster_ag=dict_clusters_agglo[5])\n",
    "\n",
    "data_fin.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689fdbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Checking the silhouette score\n",
    "print(\n",
    "    f'silhouette K-Means: {round(silhouette_score(data_fin, data_fin.cluster_km), 2)}'\n",
    ")\n",
    "print(\n",
    "    f'silhouette Agl: {round(silhouette_score(data_fin, data_fin.cluster_ag), 2)}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564c6d9d-6e3d-4f93-877c-d7b2ead9e6d9",
   "metadata": {},
   "source": [
    "A silhouette score of 0.21 for the K-Means clustering algorithm suggests that on average, objects are reasonably well clustered. A silhouette score of 0.06 for the Agglomerative Clustering algorithm suggests that the clusters are quite weak and there is considerable overlap between them. K-Means algorithm performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46547515",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculating the Davies-Bouldin Index\n",
    "db_kmeans = davies_bouldin_score(data_fin, data_fin.cluster_km)\n",
    "print(f'Davies-Bouldin K-Means: {round(db_kmeans, 2)}')\n",
    "\n",
    "db_agl = davies_bouldin_score(data_fin, data_fin.cluster_ag)\n",
    "print(f'Davies-Bouldin Agglomerative: {round(db_agl, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204c60e0-df7a-426a-a4d3-d077fdfc78d4",
   "metadata": {},
   "source": [
    "A Davies-Bouldin index of 1.61 for the K-Means clustering suggests that the clusters are moderately well-separated and compact, but there's room for improvement. A Davies-Bouldin index for Agglomerative Clustering suggests that the clusters created by this method are either very close to each other or very dispersed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a65628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Calinski-Harabasz Index\n",
    "ch_kmeans = calinski_harabasz_score(data_fin, data_fin.cluster_km)\n",
    "print(f'Calinski-Harabasz K-Means: {round(ch_kmeans, 2)}')\n",
    "\n",
    "ch_agl = calinski_harabasz_score(data_fin, data_fin.cluster_ag)\n",
    "print(f'Calinski-Harabasz Agglomerative: {round(ch_agl, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c28d6c-077a-4f71-8d1a-f1c41157cde5",
   "metadata": {},
   "source": [
    "A Calinski-Harabasz index of 27,743.29 for K-Means is quite high, suggesting that the clusters are distinct and well-separated from each other, with tight clustering within each cluster. For Agglomerative Clustering the score is lower compared to K-Means, which indicates that the clusters it formed are less dense and less well-separated. \n",
    "\n",
    "Overall, K-Means clustering algorithm outperforms Agglomerative Clustering for this dataset across all three metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e47a7c-1b9d-435f-835f-0ecfdfc779bb",
   "metadata": {},
   "source": [
    "#### Final Conclusion\n",
    "\n",
    "Using K-Means algorythm we found these typical patterns in user behaviour. Also we provide some business recomendations for every target group.\n",
    "\n",
    "**Cluster 0: New Regular Consumers**\n",
    "\n",
    "Exhibits three peaks for kWh delivered, around 5, 10, and 20 kWh. Charging duration is among the lowest, with mostly short parking without charging, but some long sessions are also present. Active from 2019-2021. Connections typically occur around 8 am, with disconnections at either 11 am or 7 pm. Charging occurs at both sites, but predominantly at Site 1 (private). **Users charge close to work for long sessions, possibly to fully charge their batteries before going home.**\n",
    "\n",
    "Value for business: time-based incentives and flexible pricing models, rewards for frequent use could encourage off-peak charging, helping to balance the grid's demand and increase station utilization during lower-demand periods.\n",
    "\n",
    "**Cluster 1: First and Flexible**\n",
    "\n",
    "Characterized by low kWh delivery, mostly under 10 kWh, with occasional peaks around 20 kWh. Charging durations are among the longest, with the longest parking durations without charging (around 4 hours). Most active in the initial years, 2018-2019. Connections vary from 6 to 10 am, predominantly at 7 am, with disconnections between 5-6 pm, across both private and public sites. **Early adopters of the service, they prefer to fully charge, possibly at stations close to their work, homes, or places where they stay for extended periods**\n",
    "\n",
    "Value for business: targeted promotions, such as loyalty rewards or discounts for consistent usage can enhance usage frequency and solidify their loyalty. Personalized communication highlighting new features or stations could further encourage engagement.\n",
    "\n",
    "**Cluster 2: Charging Professionals**\n",
    "\n",
    "Prefers charging at Site 1 (private), with high kWh deliveries (over 20), indicating either larger battery capacities or less frequent charging needs. Features the longest session durations, but short parking without charging times. Initially most active in 2018-2019, but also present in 2020-2021. **Connections typically occur between 8-9 am, with disconnections from 5-6 pm. Professionals who rely on charging stations near their workplaces.**\n",
    "\n",
    "Value for business: tailored B2B solutions and partnerships, providing dedicated charging infrastructure or reserved charging slots for employees. Off-peak pricing models for long-term parking.\n",
    "\n",
    "**Cluster 3: First Fast Public Users**\n",
    "\n",
    "Primarily uses Site 2 (public), active in 2018-2019, with the shortest kWh deliveries and charging sessions, and very short parking times without charging. Connections occur at various times, notably at 9 am and 2-3 pm, with disconnections broadly spread but mostly around 6 pm. **Occasional daily users with diverse charging needs and routines.**\n",
    "\n",
    "Value for business: targeted marketing with commercial fleets that could benefit from fast, public charging. Priority services, reserved spots during peak hours, or discounted rates for frequent users."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
